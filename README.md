# Deep Mineralogical Segmentation

This repository provides source code related to the submission of the work **Deep mineralogical segmentation of thin section images based on QEMSCAN maps** to [Computers & Geosciences](https://www.sciencedirect.com/journal/computers-and-geosciences).

Repository dedicated to the segmentation of mineral phases in carbonate thin section rocks. The proposed Computer Vision method aims at mimicking the chemical method [*QEMSCAN* (*Quantitative  Evaluation  of  Minerals by  Scanning  Electron  Microscopy*)](https://www.researchgate.net/publication/258461582_QEMSCAN_Quantitative_Evaluation_of_Minerals_by_Scanning_Electron_Microscopy_capability_and_application_to_fracture_characterization_in_geothermal_systems), which maps as image the location of the pores and different minerals present in the thin section. The current work was developed to be able to differentiate between 6 classes of mineral phases: **Calcite (0 to 1% MgO), Dolomite, Mg-Clayminerals, Pores, Quartz and Others**

The repository offers:
    
* inference with support for:
    * common image formats or netCDF files;
    * individual or batch segmentation.

* customized model training;

* evaluation of models in terms of visualization and metrics;

* comparison between different models.

## *Background*

### Network architecture

This work proposes the use of the deep semantic segmentation network [U-Net](https://arxiv.org/abs/1505.04597).

### Data

The provided resources expect both plane polarized (PP) and cross polarized (XP) light versions of high-resolution thin section images, divided into small chunks for a bigger dataset volume and a faster processing. The model input is composed of 6-channel matrices, concatenating the RGB channels of the PP image with those of the corresponding XP.

### Labeling

QEMSCAN maps corresponding to each thin section are expected to be used as groundtruth labels. Normally, the maps tend to have much lower resolution than the thin section images and not aligned to them. Therefore, all thin section images must go through a manual registration (alignment) process. As a consequence, the annotations become inaccurate in some regions, especially in transition regions between classes.

For the registration of thin section images and QEMSCAN maps, refer to the digital rocks processing platform [GeoSlicer](https://github.com/petrobras/GeoSlicer).

## Installation

```
pip install -r requirements.txt
```

**Note:** at the time of the last update of this section, the dependency [`aim`](https://github.com/aimhubio/aim), dedicated to logging experiment results, was presenting an installation problem on Windows. If this is still the case, remember to delete references to it from the `requirements.txt` file before installation and from code files that use it, such as [train_val.ipynb](train_val.ipynb).

## Use

### Training

Use the notebook [train_val.ipynb](train_val.ipynb) to trained models from datasets generated by the notebook [generate_dataset.ipynb](generate_dataset.ipynb).

### Inference

Use the Python script `inference.py` for the prediction of mineral phases on thin section images, specifying the paths to the trained models and to the input image or directory of images, the string patterns that define which images are PP or XP, among other optional arguments.

```
python inference.py --input_model ${INPUT_MODEL} --input_path ${INPUT_PATH} [--output_path ${OUTPUT_PATH}] [--pp_pattern ${PP_PATTERN}] [--px_pattern ${PX_PATTERN}] [--overwrite] [--mm_per_pixel ${MM_PER_PIXEL}]
```

* `input_model`: path to the trained model (refer to `--pp_pattern` and `--px_pattern`). Default: no path (no to segment);
* `input_path`: path to the input image or directory of images:
    * In case of a PNG or JPEG image, its type (PP/XP) is inferred based on the presence of the affixes specified by `--pp_pattern` (PP) e `--px_pattern` (XP). If the input is a single image and both affixes are specified, its counterpart is searched automatically. For instance, if `--input_path ABC_3200.5_pp.jpg --pp_pattern pp --px_pattern c2`, then the input image is recongnized as PP and it is expected that its counterpart XP also exists as `ABC_3200.5_c2.jpg`;
    * If it is a netCDF file, the inference is performed on the first PP and/or XP nodes found (PP and XP nodes are considered those that have the affixes specified by `--pp_pattern` and `--px_pattern` in their name, respectively);
    * If it is a directory, inference is executed sequentially over all PNG, JPEG and netCDF files present.
* `pp_pattern`: affix to be identified in the input image name or within the input netCDF to obtain a PP image. Default: `PP`;
* `px_pattern`: affix to be identified in the input image name or within the input netCDF to obtain a XP image. Default: no affix (do not search for XP image);
* `output_path`: path to the output directory or netCDF file. The file/directory is automatically created if it does not exist. Default: `results/`.
    * If it is a netCDF file (ending with ".nc"), a netCDF file containing the resulting predictions is saved. For inferences on PNG or JPEG images, the output file also includes the images themselves, adopting the scale specified through the `--mm_per_pixel` argument. For inferences on netCDF files, the output file includes copies of all images and segmentations present in the input file;
    * Otherwise, the argument is interpreted as a directory including the results in netCDF format (following the guidelines in the previous item).
    * The created netCDF files contain the nodes for which the affixes were specified.
* `overwrite`: if specified, output files are allowed to overwrite already existing files with the same name in the same output directory. Otherwise, both new and old files are retained;
* `mm_per_pixel`: mm/pixel scale of the image. Useful for PNG/JPEG images and ignored for netCDF files. In the latter case, the scale is obtained from the input files themselves. Default: 0.00132 mm/pixel.

### Evaluation

The [evaluation directory](evaluation/) contains notebooks for calculating [performance metrics](evaluation/metrics.ipynb) of models and for [visual comparison](evaluation/model_comparison.ipynb) of results from different models.

