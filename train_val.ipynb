{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: A imagem D_6212 foi deixada para trás durante o treinamento do modelo principal, embora conste nos arquivos de dataset (trainval.csv - treino e validação; trainfull_only.csv - treino do modelo principal), pois seus chunks não haviam sido gerados. Como os dados do segundo lote praticamente não possuem argilominerais, a imagem D_6212 (que possui) foi incluída para validação na comparação entre os seguintes modelos:  \n",
    "* *Completo*: primeiro lote validado nas imagens de validação do segundo lote (as testadas e tabeladas pelo Matheus);\n",
    "* *Novo*: primeiro lote + imagens de treino do segundo lote (todas menos as de validação);\n",
    "* *Fine-tunning*: modelo *Completo* com treinamento extra com as imagens de treino do segundo lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost-sensitive\n",
    "### Resumo:\n",
    "### * (over/under)sampling - está na lista para teste (em termos de balanceamento óptico apenas)\n",
    "### * pesos de classe:já utilizo, porém testar a configuração \n",
    "###### * melhor é o inverso da distribuição (já utilizo)\n",
    "###### * também podem ser escolhidos arbitrariamente ou buscados (grid search)\n",
    "###### * scikit-learn usa a configuração n_samples/(n_classes*n_samples_in_class), diferente da que uso (1/f = n_samples/n_samples_in_class)\n",
    "### * Emsembling - entender\n",
    "\n",
    "# [OK] WCE: corte 256 em vez de 512: algumas diferenças nos resultados, mas numericamente tão ruins quanto\n",
    "# WCE: modelo antigo 1000:8x256 batch 2 (e, se der tempo, 1000:16x256 batch 1)\n",
    "# WCE: modelo 256\n",
    "# balanceamento pelo \"Problemas_imagem\" da tabela\n",
    "# balanceamento de propriedades ópticas por oversampling em vez de probabilístico\n",
    "# aumentar as imagens simulando tingimentos\n",
    "# separar subclasses de minerais (por exemplo, calcita e calcita tingida)\n",
    "# tentar os insights do penúltimo slide de baixo para cima (exceto frequência mediana, já comprovada que é similar à frequência inversa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UwQsv99wCMB"
   },
   "source": [
    "# Exemplo de segmentação de poros com MONAI + Aim/MLFlow\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSmTplwYwCMF"
   },
   "source": [
    "## Importando módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZejZZonxwCMH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    Transform,\n",
    "    RandomizableTransform,\n",
    "    Compose,\n",
    "    EnsureType,\n",
    "    AsDiscrete,\n",
    "    Identityd,\n",
    "    LoadImaged,\n",
    "    AsChannelFirstd,\n",
    "    AddChanneld,\n",
    "    MapLabelValued,\n",
    "    ScaleIntensityRanged,\n",
    "    GaussianSmoothd,\n",
    "    CenterSpatialCropd,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandScaleIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    RandAxisFlipd,\n",
    "    RandRotate90d,\n",
    "    FillHolesd,\n",
    "    EnsureTyped,\n",
    "    ToTensord,\n",
    "    ToDeviced,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import Metric, DiceMetric\n",
    "from monai.losses import DiceLoss, DiceCELoss, DiceFocalLoss, FocalLoss, TverskyLoss, GeneralizedWassersteinDiceLoss, GeneralizedDiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import SmartCacheDataset, CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import aim\n",
    "from aim.pytorch import track_gradients_dists, track_params_dists\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import Normalize\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvkmcSGHwCMI"
   },
   "source": [
    "## Configuração do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZSoIBKEwCMI",
    "outputId": "d592b04e-87cc-4cda-a4e6-82c659d5dec8",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3EPRPqBwCMN"
   },
   "source": [
    "## Separando os arquivos de imagem para treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rgb(color_hex):\n",
    "    return (int(color_hex[1:3], 16), int(color_hex[3:5], 16), int(color_hex[5: ], 16))\n",
    "\n",
    "def generate_rgb_image(color_hex, shape):\n",
    "    image = np.zeros((*shape, 3))\n",
    "    image[:, :, :] = get_rgb(color_hex)\n",
    "    \n",
    "    return image/255.0\n",
    "\n",
    "def generate_rgb_map2(single_channel_map, is_channel_first = False, as_tensor = False):\n",
    "    rgb_map = np.zeros((*single_channel_map.shape[int(is_channel_first):], 3))\n",
    "    for i in range(element_data.index.max()):\n",
    "        element = element_data[element_data.index == i + 1]\n",
    "        single_channel_img = single_channel_map if not is_channel_first else single_channel_map[0]\n",
    "        rgb_map = np.where(\n",
    "            np.stack([single_channel_img, single_channel_img, single_channel_img], axis = 2) == i + 1,\n",
    "            generate_rgb_image(element['color_hex'].values[0], single_channel_img.shape),\n",
    "            rgb_map)\n",
    "    \n",
    "    if as_tensor:\n",
    "        rgb_map = torch.Tensor(np.rollaxis(rgb_map, rgb_map.ndim - 1, 0))\n",
    "    return rgb_map\n",
    "\n",
    "def generate_rgb_map(single_channel_map, is_channel_first = False, as_tensor = False):\n",
    "    rgb_map = np.zeros((*single_channel_map.shape[int(is_channel_first):], 3)).astype(np.uint8)\n",
    "    for i in range(element_data.index.max()):\n",
    "        element = element_data[element_data.index == i + 1]\n",
    "        single_channel_img = single_channel_map if not is_channel_first else single_channel_map[0]\n",
    "        elem_indexes = np.where(single_channel_img == i + 1)\n",
    "        rgb_map[elem_indexes[0], elem_indexes[1], :] = get_rgb(element['color_hex'].values[0]) #generate_rgb_image(element['color_hex'].values[0], single_channel_img.shape)\n",
    "    \n",
    "    if as_tensor:\n",
    "        rgb_map = torch.Tensor(np.rollaxis(rgb_map, rgb_map.ndim - 1, 0))\n",
    "    return rgb_map/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_colormap():\n",
    "    row = 1\n",
    "    nrows = element_data.shape[0]\n",
    "    for i, element in element_data.iterrows():\n",
    "        if i > 0:\n",
    "            plt.subplot(nrows, 1, row)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.ylabel(str(i) + '. ' + element['Element'], rotation = 'horizontal', horizontalalignment = 'right')\n",
    "            plt.imshow(generate_rgb_image(element['color_hex'], (30, 30)))\n",
    "            row += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_shrink(file_dicts, extension, data_is_shrank, label_is_shrank):\n",
    "    for fd in file_dicts:\n",
    "        if data_is_shrank:\n",
    "            fd['image'] = fd['image'].replace('_' + extension, '_' + extension + '_shrank')\n",
    "        if label_is_shrank:\n",
    "            fd['label'] = fd['label'].replace('_' + extension, '_' + extension + '_shrank')\n",
    "    return file_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(transform):\n",
    "    transforms_info = {}\n",
    "    for t in transform.transforms:\n",
    "        transform_function = str(t).split('.')[-1].split()[0]\n",
    "        transforms_info[transform_function] = {'keys': t.__dict__['keys'], 'values': None}\n",
    "        for key in t.__dict__:\n",
    "            if hasattr(t.__dict__[key], '__dict__'):\n",
    "                transforms_info[transform_function]['values'] = t.__dict__[key].__dict__\n",
    "    return transforms_info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_weights(data_proportions, n_files, n_groups):\n",
    "    sample_weights = []\n",
    "    if n_files > 0:\n",
    "        for entry in data_proportions['amount_by_section_id']:\n",
    "            sample_weights += entry['amount'] * [n_files/data_proportions[entry['group']]]\n",
    "    num_samples = n_groups * sample_weights.count(min(sample_weights))\n",
    "    return sample_weights, num_samples\n",
    "\n",
    "def get_group(section_id):\n",
    "    if weight_samples['by_section']:\n",
    "        return section_id[0]\n",
    "    if weight_samples['by_face']:\n",
    "        if not (data_register['Código'] == section_id).any():\n",
    "            print('ATENÇÃO:', section_id, 'não está na lista.')\n",
    "            return 'undefined'\n",
    "        face = data_register.loc[data_register['Código'] == section_id, 'Fácies'].values[0]\n",
    "        if 'shrub' in face.lower():\n",
    "            return 'shrub'\n",
    "        if face == 'FLTgst':\n",
    "            return 'GST'\n",
    "        return face[:3]\n",
    "    return ''\n",
    "\n",
    "def balance(data_files, by = 'section'):\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    assert(by in ['section', 'face'])\n",
    "    \n",
    "    X = np.array(data_files).reshape(-1, 1)\n",
    "    if by == 'section':\n",
    "        y = [path['image'].split(os.sep)[-5] for path in data_files]\n",
    "    else:\n",
    "        y = [path['group'] for path in data_files]\n",
    "    \n",
    "    return RandomOverSampler(sampling_strategy = 'not majority').fit_resample(X, y)[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_mode = False\n",
    "\n",
    "if py_mode:\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', dest = 'dataset_file', type = str)\n",
    "    parser.add_argument('--shrank-train-data', dest = 'shrank_train_data', action = 'store_true')\n",
    "    parser.add_argument('--shrank-train-label', dest = 'shrank_train_label', action = 'store_true')\n",
    "    parser.add_argument('--shrank-val-data', dest = 'shrank_val_data', action = 'store_true')\n",
    "    parser.add_argument('--shrank-val-label', dest = 'shrank_val_label', action = 'store_true')\n",
    "    parser.add_argument('--isolate-bg', dest = 'isolate_bg', action = 'store_true')\n",
    "    parser.add_argument('--include-bg-loss', dest = 'include_bg_loss', action = 'store_true')\n",
    "    parser.add_argument('--include-bg-metric', dest = 'include_bg_metric', action = 'store_true')\n",
    "    parser.add_argument('--siliciclastics', dest = 'siliciclastics_model', action = 'store_true')\n",
    "    parser.add_argument('--experiment', dest = 'experiment', type = str, default = 'test')\n",
    "    parser.add_argument('--epochs', dest = 'max_epochs', type = int, default = 50)\n",
    "    parser.add_argument('--val-interval', dest = 'val_interval', type = int, default = 50)\n",
    "    parser.add_argument('--intensity-aug', dest = 'intensity_aug', action = 'store_true')\n",
    "    parser.add_argument('--weight-samples', dest = 'weight_samples', action = 'store_true')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train_val_data_file = args.dataset_file\n",
    "    shrank = {\n",
    "        'train_data': args.shrank_train_data,  \n",
    "        'train_label': args.shrank_train_label,\n",
    "        'val_data': args.shrank_val_data,\n",
    "        'val_label': args.shrank_val_label, \n",
    "    }\n",
    "    isolate_background = args.isolate_bg\n",
    "    include_background = {\n",
    "        'loss': args.include_bg_loss,\n",
    "        'metric': args.include_bg_metric\n",
    "    }\n",
    "    siliciclastics_model = args.siliciclastics_model\n",
    "    experiment = args.experiment\n",
    "    max_epochs = args.max_epochs\n",
    "    val_interval = args.val_interval\n",
    "    intensity_aug = args.intensity_aug\n",
    "    weight_samples = args.weight_samples\n",
    "else:\n",
    "    project = 'qemscan' # qemscan / poreseg\n",
    "    train_val_data_file = os.path.join(project, 'dataset_files', 'trainval_new.csv')\n",
    "    isolate_background = False\n",
    "    shrank = {\n",
    "        'train_data': False,  \n",
    "        'train_label': False,\n",
    "        'val_data': False,\n",
    "        'val_label': False, \n",
    "    }\n",
    "    include_background = { # normally: qemscan - False/False; poreseg - True/True\n",
    "        'loss': False,\n",
    "        'metric': False\n",
    "    }\n",
    "    siliciclastics_model = False\n",
    "    experiment = 'new_data.val_matheus+d6212_argilo' # remember to change\n",
    "    max_epochs = 75\n",
    "    val_interval = 15 # None for no validation\n",
    "    intensity_aug = False\n",
    "    weight_samples = {\n",
    "        'by_section': False,\n",
    "        'by_face': False\n",
    "    }\n",
    "    balance_by_oversampling = False\n",
    "assert(weight_samples['by_section'] == False or weight_samples['by_face'] == False)\n",
    "assert(project in ['qemscan', 'poreseg'])\n",
    "    \n",
    "device = torch.device(0) if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "# use None to train a brand new model\n",
    "model_to_load = None\n",
    "show_output_comparison = False\n",
    "multigpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVZr7-kBwCMO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp_only = False # normally: qemscan - False; poreseg - True\n",
    "binary = False\n",
    "nifti = True\n",
    "exclude_pore = False\n",
    "img_size = 1000 # normally: qemscan - 1000; poreseg - 128\n",
    "crop_spatial_size = (512, 512)\n",
    "num_crops_per_img = 1\n",
    "test_background_rate_max = 5e-3\n",
    "ext = 'nii.gz' if nifti else 'tif'\n",
    "smart_cache = True\n",
    "dataset_cache_rate = 0.5\n",
    "dataset_replace_rate = 1.0\n",
    "\n",
    "if not siliciclastics_model:\n",
    "    interest_classes = ['Calcita (0% a 1%MgO)', 'Dolomita', 'Mg-Argilominerais', 'Poros', 'Quartzo', 'Outros']\n",
    "else:\n",
    "    interest_classes = ['Quartzo', 'Feldspato', 'Argilas', 'Poros', 'Outros']\n",
    "others_class = 'Outros'\n",
    "\n",
    "binary             = binary or (project == 'poreseg')\n",
    "isolate_background = isolate_background or not binary\n",
    "exclude_pore       = exclude_pore and not binary\n",
    "\n",
    "train_val_data = pd.read_csv(train_val_data_file, dtype = {'depth': str})\n",
    "data_dir = os.path.join(os.sep, 'petrobr', 'parceirosbr', 'smartseg', 'datasets', project, 'generated')\n",
    "if project == 'qemscan':\n",
    "    data_register = pd.read_csv(os.path.join('qemscan', 'register.csv'))\n",
    "\n",
    "useful_train_val_entries = train_val_data[train_val_data['train'].astype(bool) | train_val_data['val'].astype(bool)]\n",
    "\n",
    "train_files = []\n",
    "val_files   = []\n",
    "data_amounts = {'amount_by_section_id': []}\n",
    "dataset_log = {'train': [], 'val': []}\n",
    "for i, train_val_entry in useful_train_val_entries.iterrows():\n",
    "    section  = train_val_entry['section']\n",
    "    depth    = train_val_entry['depth']\n",
    "    to_train = bool(train_val_entry['train'])\n",
    "    to_val   = bool(train_val_entry['val'])\n",
    "    section_id = section + '_' + depth\n",
    "    \n",
    "    group = get_group(section_id)\n",
    "    if group == 'undefined':\n",
    "        continue\n",
    "    \n",
    "    if len(group) > 0 and group not in data_amounts:\n",
    "        data_amounts[group] = 0\n",
    "    \n",
    "    #if to_train:\n",
    "    #    dataset_log['train'].append(section_id)\n",
    "    #if to_val:\n",
    "    #    dataset_log['val'].append(section_id)\n",
    "    \n",
    "    train_val_images = sorted(\n",
    "        glob.glob(os.path.join(data_dir, section, depth,\n",
    "                               str(img_size) + 'x' + str(img_size) + '_' + ext, 'data',   \"*.\" + ext)))\n",
    "\n",
    "    train_val_labels = sorted(\n",
    "        glob.glob(os.path.join(data_dir, section, depth,\n",
    "                               str(img_size) + 'x' + str(img_size) + '_' + ext, 'labels', \"*.\" + ext)))\n",
    "    #-#labels_proportions = pd.read_csv(os.path.join(data_dir, section, depth,\n",
    "    #-#                          str(img_size) + 'x' + str(img_size) + '_' + ext, 'proportions.csv'))\n",
    "    data_dicts = [\n",
    "        {\"image\": image_name, \"label\": label_name, 'group': group}\n",
    "        for image_name, label_name in zip(train_val_images, train_val_labels)\n",
    "    ]\n",
    "    \n",
    "    if to_train and to_val:\n",
    "        val_size = int(0.2 * len(data_dicts))\n",
    "        train_files += data_dicts[:-val_size]\n",
    "        val_files   += data_dicts[-val_size:]\n",
    "        if len(group) > 0:\n",
    "            data_amounts['amount_by_section_id'].append({'amount': len(data_dicts[:-val_size]), 'group': group})\n",
    "            data_amounts[group] += data_amounts['amount_by_section_id'][-1]['amount']\n",
    "    else:\n",
    "        if to_val:\n",
    "            val_files += data_dicts\n",
    "            #-#val_proportions += list(labels_proportions.values[:, 1:])\n",
    "        else:\n",
    "            train_files += data_dicts\n",
    "            if len(group) > 0:\n",
    "                data_amounts['amount_by_section_id'].append({'amount': len(data_dicts), 'group': group})\n",
    "                data_amounts[group] += data_amounts['amount_by_section_id'][-1]['amount']\n",
    "\n",
    "    if to_train:\n",
    "        dataset_log['train'].append((section_id, len(data_dicts)))\n",
    "    if to_val:\n",
    "        dataset_log['val'].append((section_id, len(data_dicts)))\n",
    "                \n",
    "train_files = manage_shrink(train_files, ext, shrank['train_data'], shrank['train_label'])\n",
    "val_files   = manage_shrink(val_files,   ext, shrank['val_data'],   shrank['val_label'])\n",
    "\n",
    "sample_weights = None\n",
    "proportions_by_group = {}\n",
    "if weight_samples['by_section'] or weight_samples['by_face']:\n",
    "    group_type = 'section' if weight_samples['by_section'] else 'face'\n",
    "    n_groups = len(data_amounts.keys()) - 1\n",
    "    for group in data_amounts.keys():\n",
    "        if group.startswith('amount'):\n",
    "            continue\n",
    "        proportions_by_group[group] = data_amounts[group]/len(train_files)\n",
    "        \n",
    "    if balance_by_oversampling:\n",
    "        print('Balanceamento por superamostragem de grupo (' + group_type + ')...')\n",
    "        train_files = balance(train_files, by = group_type)\n",
    "        print('\\t*', len(train_files), 'de treino no total:', len(train_files)//n_groups, 'por grupo.')\n",
    "    else:\n",
    "        print('Balanceando por pesos de grupo (' + group_type + ')...')\n",
    "        sample_weights, num_samples = calculate_sample_weights(data_amounts, len(train_files), n_groups)\n",
    "        print('\\t* As', len(train_files), 'serão amostradas com probabilidade inversa à frequência do grupo, ' + \\\n",
    "            'por', num_samples, 'vezes (quantidade equivalente a de imagens por superamostragem).')\n",
    "\n",
    "len_train_log = len(dataset_log['train'])\n",
    "len_val_log   = len(dataset_log['val'])\n",
    "print('\\nConjuntos de dados\\n\\tTreino:', len(train_files), f'({len_train_log})\\n\\t\\t', dataset_log['train'], '\\n\\t\\t* Proporções por grupo:', proportions_by_group,\\\n",
    "      '\\n\\tValidação/Teste:', len(val_files), f'({len_val_log})\\n\\t\\t', dataset_log['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def class_remapping(element_data, remap_condition, target_class, target_color):\n",
    "    n_occur = element_data[remap_condition].shape[0]\n",
    "    element_data.loc[remap_condition, 'Element'] = target_class\n",
    "    element_data.loc[remap_condition, 'color_hex'] = target_color\n",
    "    \n",
    "    return element_data\n",
    "\n",
    "element_data = pd.read_csv(os.path.join(data_dir, '..', 'unified_labels.csv'), index_col = 0)\n",
    "\n",
    "orig_labels = np.arange(element_data.shape[0] + 1)\n",
    "target_labels = orig_labels.copy()\n",
    "pore_label = element_data[element_data['Element']=='Poros'].index[0]\n",
    "if binary:\n",
    "    new_pore_label = 2 if isolate_background else 1\n",
    "    new_non_pore_label = new_pore_label - 1\n",
    "\n",
    "    target_labels[np.where((orig_labels != pore_label) & (orig_labels != 0))] = new_non_pore_label\n",
    "    target_labels[np.where(orig_labels == pore_label)] = new_pore_label\n",
    "else:\n",
    "    print('Original labels...')\n",
    "    plot_colormap()\n",
    "\n",
    "    if siliciclastics_model:\n",
    "        element_data = class_remapping(element_data, element_data['Element'] == 'Albita', 'Feldspato', '#ff0000')\n",
    "        element_data = class_remapping(element_data,\n",
    "                                       (element_data['Element'] == 'Mg-Argilominerais') | \\\n",
    "                                       (element_data['Element'] == 'Caulinita') | \\\n",
    "                                       element_data['Element'].str.contains('Esmectita'), \\\n",
    "                                       'Argilas', '#00ff00')\n",
    "    element_data = class_remapping(element_data, ~element_data['Element'].isin(interest_classes), others_class, \\\n",
    "                                   element_data.loc[element_data['Element'] == others_class, 'color_hex'].values[0])\n",
    "\n",
    "    target_labels = np.array([0] + [interest_classes.index(element) + 1 for element in element_data['Element']])\n",
    "    element_data = element_data.set_index(target_labels[1:])\n",
    "\n",
    "    print('Remapped labels...')\n",
    "    plot_colormap()\n",
    "\n",
    "    element_data = element_data.set_index([target_labels[1:]]).drop_duplicates().sort_index()\n",
    "\n",
    "    print('Compacted labels...')\n",
    "    plot_colormap()\n",
    "\n",
    "elements = (['Desconhecido'] if include_background['metric'] else []) + element_data['Element'].tolist()\n",
    "n_classes = np.unique(target_labels).size\n",
    "print('Label remapping:', orig_labels, '->', target_labels)\n",
    "print('Pore label:', pore_label)\n",
    "print('Classes:', n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_binary_color_dist(image_files):\n",
    "    hist = {}\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        image = nib.load(image_file['image']).get_fdata()[:, :, :3]\n",
    "        label = nib.load(image_file['label']).get_fdata()\n",
    "        \n",
    "        for class_ in interest_classes:\n",
    "            if class_ not in hist:\n",
    "                hist[class_] = {}\n",
    "            \n",
    "            im_class = image[label == element_data[element_data['Element']==class_].index[0]]\n",
    "        \n",
    "            for channel in range(3):\n",
    "                if i == 0:\n",
    "                    hist[class_][channel] = np.histogram(im_class[:, channel], bins = 256, range = (0, 255))[0]\n",
    "                else:\n",
    "                    hist[class_][channel] = np.mean(\n",
    "                        np.append([hist[class_][channel]], [np.histogram(im_class[:, channel], bins = 256, range = (0, 255))[0]], axis = 0), axis = 0\n",
    "                    )\n",
    "        \n",
    "        if (i + 1) % (len(image_files)//10) == 0:\n",
    "            print(i + 1, '/', len(image_files))\n",
    "    \n",
    "    plt.figure(figsize = (16, 10))\n",
    "    \n",
    "    for i, class_ in enumerate(interest_classes):\n",
    "        plt_index = i + 4\n",
    "        plt.subplot(3, len(interest_classes), i + 1),                           plt.bar(range(256), hist[class_][0], color = 'red'), plt.title(class_)\n",
    "        plt.subplot(3, len(interest_classes), i + 1 + len(interest_classes)),   plt.bar(range(256), hist[class_][1], color = 'green')\n",
    "        plt.subplot(3, len(interest_classes), i + 1 + 2*len(interest_classes)), plt.bar(range(256), hist[class_][2], color = 'blue')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#check_binary_color_dist(train_files) # (160, 220), (120, 180), (80, 140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1Wi6EtAwCMO"
   },
   "source": [
    "## Experimento determinístico para reprodutibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr8HRsffwCMO"
   },
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13ZnlKGCwCMO"
   },
   "source": [
    "## Transformações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectChannelsd(Transform):\n",
    "    def __init__(self, keys, n_channels):\n",
    "        self.keys = keys\n",
    "        self.n_channels = n_channels\n",
    "    \n",
    "    def __call__(self, img_dict):\n",
    "        img_dict['image'] = img_dict['image'][:self.n_channels]\n",
    "        return img_dict\n",
    "    \n",
    "class RemoveLabelExcessd(Transform):\n",
    "    def __init__(self, keys, channel_ranges, non_pore_label, pore_label):\n",
    "        self.keys = keys\n",
    "        self.channel_ranges = channel_ranges\n",
    "        self.non_pore_label = non_pore_label\n",
    "        self.pore_label = pore_label\n",
    "    \n",
    "    def __call__(self, img_dict):\n",
    "        final_label = np.zeros(img_dict['image'].shape[-2:]) + self.non_pore_label\n",
    "        ch0t = (img_dict['image'][0] >= self.channel_ranges[0][0]) & \\\n",
    "            (img_dict['image'][0] <= self.channel_ranges[0][1])\n",
    "        ch1t = (img_dict['image'][1] >= self.channel_ranges[1][0]) & \\\n",
    "            (img_dict['image'][1] <= self.channel_ranges[1][1])\n",
    "        ch2t = (img_dict['image'][2] >= self.channel_ranges[2][0]) & \\\n",
    "            (img_dict['image'][2] <= self.channel_ranges[2][1])\n",
    "        \n",
    "        prev_label = img_dict['label'][0].copy()\n",
    "        \n",
    "        final_label[img_dict['label'][0] == 0] = 0\n",
    "        final_label[ch0t & ch1t & ch2t & (img_dict['label'][0] == self.pore_label)] = self.pore_label\n",
    "        \n",
    "        plt.figure(figsize = (36, 36))\n",
    "        imshow = np.rollaxis(np.array(img_dict['image']), 0, 3)[:, :, :3]/255\n",
    "        plt.subplot(1, 2, 1), plt.imshow(imshow)\n",
    "        imshow[img_dict['label'][0] != self.pore_label] = 0\n",
    "        plt.subplot(1, 2, 2), plt.imshow(imshow)\n",
    "        #plt.subplot(1, 2, 2), plt.imshow(img_dict['label'][0]), plt.title('QEMSCAN\\nPORO - AMARELO')\n",
    "        #plt.subplot(2, 2, 3), plt.imshow(final_label)\n",
    "        \n",
    "        img_dict['label'][0] = final_label\n",
    "        \n",
    "        #print(prev_label.all() == final_label.all())\n",
    "        \n",
    "        #plt.subplot(2, 2, 4), plt.imshow(img_dict['label'][0])\n",
    "        \n",
    "        return img_dict\n",
    "\n",
    "class Erosiond(Transform):\n",
    "    def __init__(self, keys, kernel_size, n_iter):\n",
    "        self.keys = keys\n",
    "        self.offset = kernel_size//2\n",
    "        self.n_iter = n_iter\n",
    "        self.i = 0\n",
    "    \n",
    "    def __call__(self, img_dict):\n",
    "        for i in range(self.n_iter):\n",
    "            image = img_dict['label'].copy()\n",
    "            for row in range(0, image.shape[1], 2*self.offset):\n",
    "                for col in range(0, image.shape[2], 2*self.offset):\n",
    "                    receptive_field = image[:, max(0, row-self.offset):min(image.shape[1], row+self.offset+1), max(0, col-self.offset):min(image.shape[2], col+self.offset+1)]\n",
    "                    if np.unique(receptive_field).size > 1:\n",
    "                        img_dict['label'][:, max(0, row-self.offset):min(image.shape[1], row+self.offset+1), max(0, col-self.offset):min(image.shape[2], col+self.offset+1)] = 0\n",
    "        return img_dict\n",
    "\n",
    "class Smoothd(Transform):\n",
    "    def __init__(self, keys, kernel_size, n_iter):\n",
    "        self.keys = keys\n",
    "        self.offset = kernel_size//2\n",
    "        self.n_iter = n_iter\n",
    "        self.i = 0\n",
    "    \n",
    "    def __call__(self, img_dict):\n",
    "        for i in range(self.n_iter):\n",
    "            image = img_dict['label'].clone()\n",
    "            for row in range(0, image.shape[1]):\n",
    "                for col in range(0, image.shape[2]):\n",
    "                    receptive_field = image[:, max(0, row-self.offset):min(image.shape[1], row+self.offset+1), max(0, col-self.offset):min(image.shape[2], col+self.offset+1)]\n",
    "                    if torch.unique(receptive_field).size()[0] > 1:\n",
    "                        img_dict['label'][:, row, col] = self.mode(receptive_field)\n",
    "        return img_dict\n",
    "    \n",
    "    def mode(self, array):\n",
    "        return torch.mode(array.flatten()).values.item()\n",
    "\n",
    "class RandAugmentIntensityByLabeld(RandomizableTransform):\n",
    "    def __init__(self, keys, labels, offset_ranges_per_label, min = 0, max = 255, prob = 1.0):\n",
    "        self.keys = keys\n",
    "        self.labels = labels\n",
    "        self.offset_ranges_per_label = offset_ranges_per_label\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "    \n",
    "    def __call__(self, img_dict):\n",
    "        for i, label in enumerate(self.labels):\n",
    "            offset_ranges = self.offset_ranges_per_label[i]\n",
    "            for channel in range(len(offset_ranges)):\n",
    "                offset = self.R.uniform(low = offset_range[channel][0], high = offset_range[channel][1])\n",
    "                img_dict['image'][np.where(img_dict['image'] == label)][channel] += offset\n",
    "        img_dict['image'] = np.clip(img_dict['image'], self.min, self.max)\n",
    "        return img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf7siKPOwCMO"
   },
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),                           # carregamento das imagens;\n",
    "        AsChannelFirstd(keys=[\"image\"]),                               # alterar formato de imagem para ter os canais como primeira dimensão;\n",
    "        SelectChannelsd(keys=[\"image\"], n_channels = 3 if pp_only else 6),\n",
    "        AddChanneld(keys=[\"label\"]),                      # adicionar dimensão de canal na máscara originalmente HxW;\n",
    "        MapLabelValued(keys=[\"label\"], orig_labels=orig_labels, target_labels=target_labels) \\\n",
    "            if not np.array_equal(orig_labels, target_labels) else Identityd(keys=[\"label\"]),\n",
    "        #EnsureChannelFirstd(keys=[\"image\", \"label\"]),                 # garantia que a primeira dimensão da imagem são os canais;\n",
    "        #Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),         # orientação do volume;\n",
    "        #Spacingd(keys=[\"image\", \"label\"], pixdim=(                    # fator no qual cada dimensão do volume é reduzida;\n",
    "        #    1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        #RemoveLabelExcessd(keys=[\"image\", \"label\"], channel_ranges = [(160, 220), (120, 180), (80, 140)],\n",
    "        #                 non_pore_label = new_non_pore_label, pore_label = new_pore_label) \\\n",
    "        #    if binary else Identityd(keys=[\"image\", \"label\"]),\n",
    "        #Erosiond(keys=[\"label\"], kernel_size = 9, n_iter = 1),\n",
    "        ##RandAugmentIntensityByLabeld(keys=[\"image\"], labels = range(1, n_classes),\n",
    "        ##                 offset_ranges_per_label = [\n",
    "        ##                     [(0, 0), (0, 0), (0, 0)],\n",
    "        ##                     [(0, 0), (0, 0), (0, 0)],\n",
    "        ##                     [(0, 0), (0, 0), (0, 0)],\n",
    "        ##                     [(0, 0), (0, 0), (0, 0)],\n",
    "        ##                     [(0, 0), (0, 0), (0, 0)],\n",
    "        ##                     [(0, 0), (0, 0), (0, 0)]\n",
    "        ##                 ],\n",
    "        ##prob = 0.5),\n",
    "        #FillHolesd(keys=['label'], applied_labels = new_pore_label) \\\n",
    "        #    if binary else FillHolesd(keys=['label'], applied_labels = None, connectivity = 1),\n",
    "        ScaleIntensityRanged(                                          # normalização;\n",
    "            keys=[\"image\"], a_min=0, a_max=255,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        #-$RandCropByPosNegLabeld(                                        # cortes aleatórios na imagem para aumento de dados;\n",
    "        #-$    keys=[\"image\", \"label\"],\n",
    "        #-$    label_key=\"label\",\n",
    "        #-$    spatial_size=crop_spatial_size,\n",
    "        #-$    pos=1,\n",
    "        #-$    neg=1,\n",
    "        #-$    num_samples=num_crops_per_img,\n",
    "        #-$    image_key=\"image\",\n",
    "        #-$    image_threshold=0\n",
    "        #-$),\n",
    "        #CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),  # descarte de background nos entornos;\n",
    "        #HistogramNormalized(keys=[\"image\"], num_bins = 5, max = 1),\n",
    "        \n",
    "        RandSpatialCropSamplesd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=crop_spatial_size,\n",
    "            num_samples=num_crops_per_img,\n",
    "            random_size=False\n",
    "        ) if img_size > 256 else Identityd(keys=[\"image\", \"label\"]),\n",
    "        \n",
    "        #EnsureTyped(keys=[\"image\", \"label\"]),#ToTensord(keys=['image','label']),\n",
    "        #ToDeviced(keys=['image','label'], device = 'cuda:0'),\n",
    "        #Smoothd(keys=[\"label\"], kernel_size = 7, n_iter = 3),\n",
    "        #EnsureTyped(keys=[\"image\", \"label\"]),#ToTensord(keys=['image','label']),\n",
    "        #ToDevice(keys=['image','label'], device = 'cpu'),\n",
    "        \n",
    "        #RandCropByLabelClassesd(\n",
    "        #    keys=[\"image\", \"label\"],\n",
    "        #    spatial_size=crop_spatial_size,\n",
    "        #    image_key='image',\n",
    "        #    label_key='label',\n",
    "        #    num_classes=n_classes,\n",
    "        #    ratios=[1, 1, 1, 10, 1, 1, 10]\n",
    "        #),\n",
    "        #RandSmoothDeformd(keys=['label'], spatial_size = crop_spatial_size, rand_size = 100),\n",
    "        RandScaleIntensityd(keys=[\"image\"], factors = 0.25, prob = 0.3) if intensity_aug else Identityd(keys=[\"image\"]),\n",
    "        RandAdjustContrastd(keys=[\"image\"], prob = 0.3) if intensity_aug else Identityd(keys=[\"image\"]),\n",
    "        #RandGaussianSmoothd(keys=[\"image\"], prob = 0.3),\n",
    "        RandAxisFlipd(keys=[\"image\", \"label\"], prob = 0.3),\n",
    "        #RandZoomd(keys=[\"image\", \"label\"]),\n",
    "        RandRotate90d(keys=[\"image\", \"label\"], prob = 0.3),\n",
    "        # RandAffined(                                                 # transformações aleatórias customizáveis;\n",
    "        #     keys=['image', 'label'],\n",
    "        #     mode=('bilinear', 'nearest'),\n",
    "        #     prob=1.0, spatial_size=(96, 96, 96),\n",
    "        #     rotate_range=(0, 0, np.pi/15),\n",
    "        #     scale_range=(0.1, 0.1, 0.1)),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),                          # assegura que os dados estão em formato compatível.\n",
    "    ]\n",
    ")\n",
    "        \n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AsChannelFirstd(keys=[\"image\"]),\n",
    "        SelectChannelsd(keys=[\"image\"], n_channels = 3 if pp_only else 6),\n",
    "        AddChanneld(keys=[\"label\"]),\n",
    "        MapLabelValued(keys=[\"label\"], orig_labels=orig_labels, target_labels=target_labels) \\\n",
    "            if not np.array_equal(orig_labels, target_labels) else Identityd(keys=[\"label\"]),\n",
    "        #EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        #Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(keys=[\"image\", \"label\"], pixdim=(\n",
    "        #    1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        #RemoveLabelExcessd(keys=[\"image\", \"label\"], channel_ranges = [(160, 220), (120, 180), (80, 140)],\n",
    "        #                 non_pore_label = new_non_pore_label, pore_label = new_pore_label) \\\n",
    "        #    if binary else Identityd(keys=[\"image\", \"label\"]), \n",
    "        #FillHolesd(keys=['label'], applied_labels = new_pore_label) \\\n",
    "        #    if binary else FillHolesd(keys=['label'], applied_labels = None, connectivity = 1),\n",
    "        #Erosiond(keys=[\"label\"], kernel_size = 9, n_iter = 1),\n",
    "\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=0, a_max=255,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        #0#CenterSpatialCropd(\n",
    "        #0#    keys=[\"image\", \"label\"],\n",
    "        #0#    roi_size=crop_spatial_size,\n",
    "        #0#) if img_size > 256 else Identityd(keys=[\"image\", \"label\"]),\n",
    "        \n",
    "        #EnsureTyped(keys=[\"image\", \"label\"]),#ToTensord(keys=['image','label']),\n",
    "        #ToDeviced(keys=['image','label'], device = 'cuda:0'),\n",
    "        #Smoothd(keys=[\"label\"], kernel_size = 7, n_iter = 3),\n",
    "        #EnsureTyped(keys=[\"image\", \"label\"]),#ToTensord(keys=['image','label']),\n",
    "        #ToDevice(keys=['image','label'], device = 'cpu'),\n",
    "        \n",
    "        #CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        #HistogramNormalized(keys=[\"image\"], num_bins = 5, max = 1),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        #adaptor(RemoveLabelExcess(), {'image': 'image', 'label': 'label'})\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "au4rmQfDwCMP"
   },
   "source": [
    "## Pré-visualização de imagem e rótulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dense_proportions(elements, dense_proportions):\n",
    "    for key in ['true', 'pred']:\n",
    "        dense_proportions[key] = dense_proportions[key].cpu().numpy()\n",
    "    \n",
    "    for i, element in enumerate(elements):\n",
    "        bar_color = element_data[element_data['Element'] == element]['color_hex'].values[0] if element != 'Desconhecido' else 'black'\n",
    "        plt.bar(x = element, height = dense_proportions['true'][i], color = bar_color, align = 'edge', width = -0.4, edgecolor = 'blue')\n",
    "        plt.bar(x = element, height = dense_proportions['pred'][i], color = bar_color, align = 'edge', width =  0.4, edgecolor = 'red')\n",
    "    plt.xticks(rotation = 'vertical')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.legend(\n",
    "        handles = [\n",
    "            plt.plot([], label = 'Real', color = 'blue')[0],\n",
    "            plt.plot([], label = 'Predicted', color = 'red')[0]\n",
    "        ]\n",
    "    )\n",
    "    fig = plt.figure()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Proportion(Metric):\n",
    "    def __init__(self, n_classes, include_background = True, argmax = True):\n",
    "        self.n_classes = n_classes\n",
    "        self.include_background = include_background\n",
    "        self.argmax = argmax\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def __call__(self, y_pred, y, image_path):\n",
    "        if self.argmax:\n",
    "            y_1ch      = torch.argmax(y[0],      dim = 0).to(device)\n",
    "            y_pred_1ch = torch.argmax(y_pred[0], dim = 0).to(device)\n",
    "        else:\n",
    "            y_1ch      = y.int().to(device)\n",
    "            y_pred_1ch = y_pred.int().to(device)   \n",
    "        \n",
    "        image_path_components = image_path.split(os.sep)\n",
    "        section_id = image_path_components[-5] + '_' + image_path_components[-4]\n",
    "        \n",
    "        self.true = self.__calculate(y_1ch,      self.true, section_id)\n",
    "        self.pred = self.__calculate(y_pred_1ch, self.pred, section_id)\n",
    "    \n",
    "    def __calculate(self, input, output, section_id):\n",
    "        if not self.include_background:\n",
    "            input = input[input != 0]\n",
    "        \n",
    "        proportions = torch.bincount(input.flatten(), minlength = self.n_classes) / torch.numel(input)\n",
    "        proportions = proportions.reshape(1, -1)\n",
    "        if section_id not in output:\n",
    "            output[section_id] = proportions\n",
    "        else:\n",
    "            output[section_id] = torch.cat((output[section_id], proportions), dim = 0)\n",
    "        return output\n",
    "    \n",
    "    def aggregate(self):\n",
    "        first_class = int(not self.include_background)\n",
    "\n",
    "        metrics = {'total': {}}\n",
    "        section_ids = self.true.keys()\n",
    "        for section_id in section_ids:\n",
    "            metrics[section_id] = {\n",
    "                'true': self.true[section_id].mean(dim = 0)[first_class:],\n",
    "                'pred': self.pred[section_id].mean(dim = 0)[first_class:],\n",
    "            }\n",
    "\n",
    "        metrics['total']['true'] = torch.mean(\n",
    "            torch.cat(tuple([metrics[section_id]['true'].reshape(1, -1) for section_id in section_ids]), dim = 0), dim = 0).cpu()\n",
    "        metrics['total']['pred'] = torch.mean(\n",
    "            torch.cat(tuple([metrics[section_id]['pred'].reshape(1, -1) for section_id in section_ids]), dim = 0), dim = 0).cpu()\n",
    "        \n",
    "        X, y = metrics['total']['pred'].reshape(-1, 1), metrics['total']['true'].reshape(-1, 1)\n",
    "        linear_regression = LinearRegression().fit(X, y)\n",
    "\n",
    "        metrics['total']['R2'] = linear_regression.score(X, y)\n",
    "        metrics['total']['RMSE'] = mean_squared_error(y, linear_regression.predict(X), squared = False)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def reset(self):\n",
    "        self.true = {}\n",
    "        self.pred = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportions = np.array([0.4124453,  0.18595047, 0.06884056, 0.04539315, 0.2623897,  0.02498083]) #np.array([0.41365188, 0.18531045, 0.06884208, 0.04491788, 0.26241738, 0.02486032]) #np.array([0.4019435,  0.20186418, 0.06687519, 0.04392304, 0.25787783, 0.02751627])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "qqcFPuVkwCMP",
    "outputId": "4189428e-4569-4453-e379-df4466208c85",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "seed = np.random.randint(10000)\n",
    "print(seed)\n",
    "torch.manual_seed(seed)\n",
    "check_loader = DataLoader(check_ds, batch_size=1, shuffle = True)\n",
    "check_data = first(check_loader)\n",
    "print('check_data[\\\"image\\\"].shape = ', check_data[\"image\"].shape)\n",
    "print('check_data[\\\"label\\\"].shape = ', check_data[\"label\"].shape)\n",
    "for i in range(check_data[\"image\"].shape[0]):\n",
    "    image, label = (check_data[\"image\"][i], check_data[\"label\"][i])\n",
    "    image_pp = np.rollaxis(np.array(image), 0, 3)[:, :, :3] # colocando os canais como última dimensão\n",
    "    image_px = np.rollaxis(np.array(image), 0, 3)[:, :, 3:]\n",
    "    print(i, ')', f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "    plt.figure(\"check\", (12, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image_pp)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"label\")\n",
    "    #plt.imshow(np.where(np.stack([label[0], label[0], label[0]], axis = 2) != pore_label, image_pp, (image_pp + 1)/2))\n",
    "    label_to_show = generate_rgb_map((label[0]))\n",
    "    print(label_to_show.min(), label_to_show.mean(), label_to_show.max())\n",
    "    plt.imshow((label_to_show + image_pp)/2.0)\n",
    "    if not pp_only:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"image PX\")\n",
    "        plt.imshow(image_px)\n",
    "    plt.show()\n",
    "print(elements)\n",
    "if train_proportions is None:\n",
    "    print('Calculando proporções dos elementos nos dados de treino...')\n",
    "    check_ds = Dataset(data=train_files, transform=val_transforms)\n",
    "    check_loader = DataLoader(check_ds, batch_size=1, shuffle = False)\n",
    "    train_proportions = Proportion(n_classes=n_classes, include_background=include_background['metric'], argmax=False)\n",
    "    for i, data in enumerate(check_loader):\n",
    "        if np.random.uniform() < 0.01:\n",
    "            print(int(100*i/(len(check_ds)//check_loader.batch_size)), '%', end = ' == ')\n",
    "        train_proportions(y_pred = data['label'], y = data['label'], image_path = data['image_meta_dict']['filename_or_obj'][0])\n",
    "    train_proportions = train_proportions.aggregate()\n",
    "    plot_dense_proportions(elements, train_proportions['total'])\n",
    "    train_proportions = train_proportions['total']['true']\n",
    "    print(train_proportions)\n",
    "else:\n",
    "    print('ATENÇÃO: as proporções já estão calculadas. Usando:')\n",
    "    print()\n",
    "    print(train_proportions)\n",
    "    plot_dense_proportions(elements, {'true': torch.Tensor(train_proportions), 'pred': torch.Tensor(len(train_proportions) * [0])})\n",
    "    print('Para recalculá-las, execute a célula que atribui None à variável train_proportions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hist_check_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        MapLabelValued(keys=[\"label\"], orig_labels=orig_labels, target_labels=target_labels) \\\n",
    "            if not np.array_equal(orig_labels, target_labels) else Identityd(keys=[\"label\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image_files = train_files\n",
    "bins = 5\n",
    "#def check_binary_color_dist(image_files, bins):\n",
    "check_ds = Dataset(data = image_files, transform = hist_check_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size = 1, shuffle = False)\n",
    "\n",
    "hist = {}\n",
    "for i, data in enumerate(check_loader):\n",
    "    image = data['image'][0, :, :, :3]\n",
    "    label = data['label'][0]\n",
    "\n",
    "    for class_ in interest_classes:\n",
    "        if class_ not in hist:\n",
    "            hist[class_] = {}\n",
    "\n",
    "        im_class = image[label == element_data[element_data['Element']==class_].index[0]]\n",
    "\n",
    "        if i == 0:\n",
    "            hist[class_] = np.histogramdd(im_class, bins = bins, range = [(0, 255), (0, 255), (0, 255)])[0]\n",
    "        else:\n",
    "            hist[class_] = np.mean(\n",
    "                np.append([hist[class_]], [np.histogramdd(im_class, bins = bins, range = [(0, 255), (0, 255), (0, 255)])[0]], axis = 0), axis = 0\n",
    "            )\n",
    "\n",
    "    if (i + 1) % (len(image_files)//10) == 0:\n",
    "        print(i + 1, '/', len(image_files))\n",
    "\n",
    "for i, class_ in enumerate(interest_classes):\n",
    "    ax = plt.figure(figsize = (6, 6)).add_subplot(projection = '3d')\n",
    "    ax.set_title(class_)\n",
    "    ax.set_xlabel('R')\n",
    "    ax.set_ylabel('G')\n",
    "    ax.set_zlabel('B')\n",
    "\n",
    "    binsR, binsG, binsB = np.meshgrid(range(bins), range(bins), range(bins))\n",
    "    ax.scatter3D(binsR, binsG, binsB, s = 1000*hist[class_][binsR, binsG, binsB]/hist[class_][binsR, binsG, binsB].max())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#check_binary_color_dist(train_files, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct_files = [file for file in train_files if ('A/5242.95' in file['image'])]\n",
    "check_ds = Dataset(data=correct_files, transform=val_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1, shuffle = False)\n",
    "for i, check_data in enumerate(check_loader):\n",
    "    if i not in [3, 4, 14, 15, 27, 39, 51, 63, 75, 86, 98, 99, 110, 122, 123, 134, 135, 145, 146]:\n",
    "        continue\n",
    "    \n",
    "    image, label = (check_data[\"image\"][0], check_data[\"label\"][0])\n",
    "    image_pp = np.rollaxis(np.array(image), 0, 3)[:, :, :3] # colocando os canais como última dimensão\n",
    "    image_px = np.rollaxis(np.array(image), 0, 3)[:, :, 3:]\n",
    "    print(i, ')', f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "    print(check_data['image_meta_dict']['filename_or_obj'][0])\n",
    "    plt.figure(\"check\", (12, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image_pp)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"label\")\n",
    "    #plt.imshow(np.where(np.stack([label[0], label[0], label[0]], axis = 2) != pore_label, image_pp, (image_pp + 1)/2))\n",
    "    if binary:\n",
    "        plt.imshow(label[0], norm = Normalize(0, target_labels.max()))\n",
    "    else:\n",
    "        label_to_show = generate_rgb_map(label[0])\n",
    "        plt.imshow((label_to_show + image_pp)/2.0)\n",
    "    if not pp_only:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"image PX\")\n",
    "        plt.imshow(image_px)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOgy1x1BwCMQ"
   },
   "source": [
    "## Definindo modelo e _loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix(Metric):\n",
    "    def __init__(self, n_classes, include_background = True, reduction = 'mean'):\n",
    "        assert(reduction in ['mean', 'norm_mean', 'sum'])\n",
    "        self.first_class = int(not include_background)\n",
    "        \n",
    "        self.n_classes = n_classes - self.first_class\n",
    "        self.include_background = include_background\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def __call__(self, y_pred, y, image_path):\n",
    "        self.cumul_matrices += 1\n",
    "        \n",
    "        y_1ch      = torch.argmax(y[0],      dim = 0).to(device)\n",
    "        y_pred_1ch = torch.argmax(y_pred[0], dim = 0).to(device)\n",
    "        \n",
    "        image_path_components = image_path.split(os.sep)\n",
    "        section_id = image_path_components[-5] + '_' + image_path_components[-4]\n",
    "        \n",
    "        if section_id not in self.matrix:\n",
    "            self.matrix[section_id] = torch.zeros(self.n_classes, self.n_classes).to(device)\n",
    "        \n",
    "        for actual_class in range(self.first_class, self.n_classes + self.first_class):\n",
    "            for pred_class in range(self.first_class, self.n_classes + self.first_class):\n",
    "                self.matrix[section_id][actual_class - self.first_class][pred_class - self.first_class] += \\\n",
    "                    ((y_1ch == actual_class) & (y_pred_1ch == pred_class)).count_nonzero()\n",
    "        \n",
    "        return self.matrix\n",
    "    \n",
    "    def aggregate(self):\n",
    "        sections = self.matrix.keys()\n",
    "        self.matrix['total'] = torch.zeros(self.n_classes, self.n_classes).to(device)\n",
    "        for section_id in sections:\n",
    "            self.matrix['total'] += self.matrix[section_id]\n",
    "        \n",
    "        if 'mean' in self.reduction:\n",
    "            for key in self.matrix:\n",
    "                self.matrix[key] = self.matrix[key] / self.cumul_matrices\n",
    "                if self.reduction == 'norm_mean':\n",
    "                    self.matrix[key] = self.matrix[key] / self.matrix[key].sum(dim = 1, keepdims = True)\n",
    "        return self.matrix\n",
    "    \n",
    "    def reset(self):\n",
    "        self.matrix = {}\n",
    "        self.cumul_matrices = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0_EHJ7FwCMQ"
   },
   "source": [
    "## Definindo Dataset e DataLoader para treino e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKA4gboPwCMQ",
    "outputId": "2496df99-8445-4c70-a3b1-721f9e552b34",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_gpus = 1# if not multigpu else len(model.device_ids)\n",
    "\n",
    "if smart_cache:\n",
    "    train_ds = SmartCacheDataset(data = train_files, transform = train_transforms, cache_rate = dataset_cache_rate,\n",
    "                                 replace_rate = dataset_replace_rate)\n",
    "else:\n",
    "    train_ds = CacheDataset(data = train_files, transform = train_transforms, cache_rate = dataset_cache_rate)\n",
    "\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "# to generate 2 x 4 images for network training\n",
    "train_loader = DataLoader(train_ds, batch_size=16*n_gpus, shuffle = sample_weights is None,\n",
    "                         sampler = WeightedRandomSampler(sample_weights, num_samples, replacement = True) if sample_weights is not None else None)\n",
    "\n",
    "val_ds = CacheDataset(\n",
    "    data=val_files, transform=val_transforms, cache_rate=1.0)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=n_gpus)#,\n",
    "                         #sampler = torch.utils.data.distributed.DistributedSampler(val_ds))#, num_workers=4)\n",
    "\n",
    "train_loader.sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from monai.networks.nets import FlexibleUNet\n",
    "\n",
    "UNet_metadata = dict(\n",
    "    in_channels = (3 if pp_only else 6),\n",
    "    out_channels = n_classes,\n",
    "    backbone = 'efficientnet-b0',\n",
    "    pretrained = True,\n",
    "    spatial_dims = 2,\n",
    "    decoder_channels = tuple(reversed((16, 32, 64, 128, 256))),\n",
    "    norm = Norm.BATCH,\n",
    "    act = 'prelu',\n",
    "    decoder_bias = True\n",
    ")\n",
    "model = FlexibleUNet(**UNet_metadata).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from monai.networks.nets import UNETR\n",
    "\n",
    "in_channels = (3 if pp_only else 6)\n",
    "UNet_metadata = dict(\n",
    "    in_channels = in_channels,\n",
    "    out_channels = n_classes,\n",
    "    img_size = crop_spatial_size,\n",
    "    spatial_dims = 2,\n",
    "    norm_name = Norm.BATCH,\n",
    ")\n",
    "model = UNETR(**UNet_metadata).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VM-5g2bmwCMQ"
   },
   "outputs": [],
   "source": [
    "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
    "##device = torch.device(\"cuda:1\")\n",
    "\n",
    "#import process\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "#def gpu_process(rank, world_size):\n",
    "#    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "#    os.environ['MASTER_PORT'] = '8891'\n",
    "#    torch.distributed.init_process_group('nccl', world_size = world_size, rank = rank)\n",
    "\n",
    "UNet_metadata = dict(\n",
    "    spatial_dims=2,\n",
    "    in_channels=(3 if pp_only else 6),\n",
    "    out_channels=n_classes, # probabilidade de classe em cada canal\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),#\n",
    "    #channels=(8, 16, 32, 64, 128, 256, 512, 1024),\n",
    "    #strides=7*(2,),#\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    "    #kernel_size = 9,\n",
    "    #up_kernel_size = 9\n",
    ")\n",
    "\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True).to(device)\n",
    "#model.backbone.conv1 = torch.nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device)\n",
    "model = UNet(**UNet_metadata).to(device)\n",
    "\n",
    "if multigpu:\n",
    "    world_size = 4\n",
    "    #print('Bf spw')\n",
    "    mp.spawn(process.gpu_process, args = (world_size,), nprocs = world_size, join = True)\n",
    "    #print('Af spw')\n",
    "    #torch.distributed.init_process_group('nccl', world_size = 4, rank = 0)\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    #print('Af dist')\n",
    "    #model = torch.nn.parallel.DataParallel(model)\n",
    "loss_function = DiceLoss(include_background=include_background['loss'], to_onehot_y=True, softmax=True)\n",
    "loss_type = \"DiceLoss\"\n",
    "\n",
    "#loss_function = DiceCELoss(include_background=include_background['loss'], to_onehot_y=True, softmax=True,\n",
    "#                          ce_weight = torch.cat((torch.Tensor([0]), torch.Tensor(1/train_proportions))).to(device),\n",
    "#                          #ce_weight = torch.Tensor([0, 1, 1, 0, 1, 1, 0]).to(device),\n",
    "#                          lambda_dice = 0)\n",
    "#loss_type = \"DiceCELoss\"\n",
    "\n",
    "##experiment = 'cbn-focal-instnorm\n",
    "#loss_function = FocalLoss(include_background=include_background['loss'], to_onehot_y=True,\n",
    "#                         weight = torch.Tensor(1/train_proportions)).to(device)\n",
    "#loss_type = \"FocalLoss\"\n",
    "#experiment = 'cbn-dicefocal-fulldata-lessfocal'\n",
    "#loss_function = DiceFocalLoss(include_background=include_background['loss'], to_onehot_y=True,\n",
    "#                         focal_weight = torch.Tensor(1/train_proportions), lambda_focal = 0.02).to(device)\n",
    "#loss_type = \"DiceFocalLoss\"\n",
    "###experiment = 'cbn-tversky'\n",
    "##loss_function = TverskyLoss(include_background=include_background['loss'], to_onehot_y=True, softmax=True)\n",
    "##loss_type = \"TverskyLoss\"\n",
    "####max_epochs = 10\n",
    "#experiment = 'cbn-gendice-fulldata-slower'\n",
    "#loss_function = GeneralizedDiceLoss(include_background=include_background['loss'], to_onehot_y=True, softmax=True)\n",
    "#loss_type = \"GeneralizedDiceLoss\"\n",
    "#dist_matrix = torch.Tensor(\n",
    "#    [\n",
    "#        [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "#        [1.0, 0.0, 0.02, 0.3, 1.0, 0.02, 0.9],\n",
    "#        [1.0, 0.02, 0.0, 0.3, 1.0, 0.02, 0.9],\n",
    "#        [1.0, 0.3, 0.3, 0.0, 1.0, 0.3, 0.9],\n",
    "#        [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0],\n",
    "#        [1.0, 0.02, 0.02, 0.3, 1.0, 0.0, 0.9],\n",
    "#        [1.0, 0.9, 0.9, 0.9, 1.0, 0.9, 0.0]\n",
    "#    ]\n",
    "#).to(device)\n",
    "#experiment = 'cbn-gwd-fulldata'\n",
    "#loss_function = GeneralizedWassersteinDiceLoss(dist_matrix)\n",
    "#loss_type = \"GeneralizedWassersteinDiceLoss\"\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = 1e-4, momentum = 0.9, nesterov = True)\n",
    "dice_metric = DiceMetric(include_background=include_background['metric'], reduction=\"mean\")\n",
    "confusion_metric = ConfusionMatrix(n_classes=n_classes, include_background=include_background['metric'], reduction='norm_mean')\n",
    "proportion_metric = Proportion(n_classes=n_classes, include_background=include_background['metric'], argmax=True) #torch.nn.KLDivLoss(reduction = 'mean')\n",
    "\n",
    "print(loss_type)\n",
    "\n",
    "Optimizer_metadata = {}\n",
    "for ind, param_group in enumerate(optimizer.param_groups):\n",
    "    optim_meta_keys = list(param_group.keys())\n",
    "    Optimizer_metadata[f'param_group_{ind}'] = {key: value for (key, value) in param_group.items() if 'params' not in key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nD1pAY-wCMR"
   },
   "source": [
    "## Treino e validação em PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(image, title):\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    image = tensor.cpu().numpy()\n",
    "    if tensor.ndim == 3:\n",
    "        image = np.rollaxis(image, 0, 3)\n",
    "    return image\n",
    "\n",
    "def get_comparison(input_tensor, label_tensor, pred_tensor, confidence_map, step, show = False):\n",
    "    if binary:\n",
    "        #input_label = torch.where(label_tensor.to(input_tensor.device) == new_pore_label, (input_tensor + 1)/2, input_tensor)\n",
    "        #input_pred  = torch.where(pred_tensor.to(input_tensor.device)  == 1,              (input_tensor + 1)/2, input_tensor)\n",
    "        label_tensor = label_tensor.to(input_tensor.device)\n",
    "        pred_tensor  = pred_tensor.to (input_tensor.device)\n",
    "        \n",
    "        true_positives = torch.zeros(input_tensor.shape)\n",
    "        true_positives[1] = 1\n",
    "        false_positives = torch.zeros(input_tensor.shape)\n",
    "        false_positives[0] = 1\n",
    "        false_negatives = torch.zeros(input_tensor.shape)\n",
    "        false_negatives[:2] = 1\n",
    "        \n",
    "        input_label = torch.where(label_tensor.to(input_tensor.device) == new_pore_label, (input_tensor + true_positives)/2, input_tensor)\n",
    "        input_pred  = torch.where((pred_tensor == 1) & (pred_tensor.to(input_tensor.device) == label_tensor.to(input_tensor.device)), (input_tensor + true_positives)/2, input_tensor)\n",
    "        input_pred  = torch.where((pred_tensor == 1) & (pred_tensor.to(input_tensor.device) != label_tensor.to(input_tensor.device)), (input_tensor + false_positives)/2, input_pred)\n",
    "        input_pred  = torch.where((pred_tensor == 0) & (pred_tensor.to(input_tensor.device) != label_tensor.to(input_tensor.device)), (input_tensor + false_negatives)/2, input_pred)\n",
    "    else:\n",
    "        #input_label = label_tensor\n",
    "        #input_pred  =  pred_tensor\n",
    "        input_label = (input_tensor + label_tensor)/2\n",
    "        input_pred  = (input_tensor +  pred_tensor)/2\n",
    "    input_false = (label_tensor != pred_tensor).to(torch.float)\n",
    "        \n",
    "    input_image = tensor_to_image(input_tensor)\n",
    "    input_label = tensor_to_image(input_label)\n",
    "    input_pred  = tensor_to_image(input_pred)\n",
    "    input_conf  = tensor_to_image(confidence_map)\n",
    "    input_false = tensor_to_image(input_false)\n",
    "    \n",
    "    #frames.append(plot(input_image, 'original'))\n",
    "    #frames.append(plot(input_label, 'label'))\n",
    "    #frames.append(plot(input_image, 'original'))\n",
    "    #frames.append(plot(input_pred,  'prediction'))\n",
    "    \n",
    "    #gif.save(frames, 'foo.gif', duration = 2000)\n",
    "    #Image.open('foo.gif').show()\n",
    "    \n",
    "    fig = plt.figure(figsize = (16, 16))\n",
    "    plt.subplot(2, 2, 1), plt.title('ORIGINAL'),   plt.axis('off'), plt.imshow(input_image)\n",
    "    plt.subplot(2, 2, 2), plt.title('LABEL'),      plt.axis('off'), plt.imshow(input_label)    \n",
    "    plt.subplot(2, 2, 3), plt.title('PREDICTION'),   plt.axis('off'), plt.imshow(input_pred)\n",
    "    plt.legend(\n",
    "        handles = [\n",
    "            plt.plot([], label = 'True positives', color = 'green')[0],\n",
    "            plt.plot([], label = 'False positives', color = 'red')[0],\n",
    "            plt.plot([], label = 'False negatives', color = 'yellow')[0]\n",
    "        ],\n",
    "        loc = 'best',\n",
    "        fontsize = 'x-large'\n",
    "    )\n",
    "    plt.subplot(2, 2, 4), plt.title('CONFIDENCE'), plt.axis('off'), plt.imshow(input_conf, cmap = 'gray')\n",
    "    #plt.subplot(2, 2, 4), plt.title('FALSES'), plt.axis('off'), plt.imshow(input_false, cmap = 'gray')\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    output = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "    if show:\n",
    "        #os.makedirs('images', exist_ok = True)\n",
    "        #plt.savefig(os.path.join('images', str(step) + '.png'))\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, best_metric = -1, best_metric_epoch = -1, validation = False):\n",
    "    def is_step_last_or_multiple_of(step, total_steps, multiple_of):\n",
    "        return (step % multiple_of == 0) or step == total_steps\n",
    "    \n",
    "    if validation:\n",
    "        print('Validating...')\n",
    "        prefix = 'val_'\n",
    "    else:\n",
    "        print('Testing...')\n",
    "        prefix = 'test_'\n",
    "    \n",
    "    post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=n_classes)])\n",
    "    post_label = Compose([EnsureType(), AsDiscrete(to_onehot=n_classes)])\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    step = 0\n",
    "    time_secs = {'Sliding Window Inference': [], 'Aim Image Logging': [], 'Loss calculation': [], \\\n",
    "                 'Dice metric': [], 'Proportion metric': [], 'Confusion metric': [], \\\n",
    "                 'Dice aggregation': 0, 'Proportion aggregation': 0, 'Confusion aggregation': 0, \\\n",
    "                 'Loading': [], 'Plot': 0, 'Model saving': 0, 'Non-zero counting': [], 'To GPU': [], \\\n",
    "                 'Output and conf. map': [], 'RGB mapping': [], 'Metrics logging': 0, 'Total': 0}\n",
    "    with torch.no_grad():\n",
    "        time_secs['Total'] = time.time()\n",
    "        time_secs['Loading'].append(time.time())\n",
    "        for index, test_data in enumerate(tqdm(test_loader)):\n",
    "            time_secs['Loading'][-1] = time.time() - time_secs['Loading'][-1]\n",
    "            \n",
    "            test_inputs, test_labels = (\n",
    "                test_data[\"image\"],##.to(device),\n",
    "                test_data[\"label\"],##.to(device),\n",
    "            )\n",
    "            \n",
    "            if test_inputs.shape[0] > 1:\n",
    "                test_inputs = test_inputs[:1, :, :, :]\n",
    "                test_labels = test_labels[:1, :, :, :]\n",
    "            \n",
    "            time_secs['Non-zero counting'].append(time.time())\n",
    "            background_rate = 1 - torch.count_nonzero(test_data[\"label\"])/torch.numel(test_data[\"label\"])\n",
    "            time_secs['Non-zero counting'][-1] = time.time() - time_secs['Non-zero counting'][-1]\n",
    "            if (not binary or isolate_background) and background_rate > test_background_rate_max:\n",
    "                time_secs['Loading'].append(time.time())\n",
    "                continue\n",
    "            #test_inputs, test_labels = test_inputs[0].reshape([1, -1, 512, 512]), test_labels[0].reshape([1, -1, 512, 512])\n",
    "            time_secs['To GPU'].append(time.time())\n",
    "            if not multigpu:\n",
    "                test_inputs = test_inputs.to(device)\n",
    "                test_labels = test_labels.to(device)\n",
    "            time_secs['To GPU'][-1] = time.time() - time_secs['To GPU'][-1]\n",
    "            \n",
    "            step += 1\n",
    "            image_path = test_data['image_meta_dict']['filename_or_obj'][0]\n",
    "            \n",
    "            roi_size = crop_spatial_size\n",
    "            sw_batch_size = 4\n",
    "            time_secs['Sliding Window Inference'].append(time.time())\n",
    "            test_outputs = sliding_window_inference(\n",
    "                test_inputs, roi_size, sw_batch_size, model)\n",
    "            time_secs['Sliding Window Inference'][-1] = time.time() - time_secs['Sliding Window Inference'][-1]\n",
    "\n",
    "            # tracking input, label and output images with Aim\n",
    "            time_secs['Output and conf. map'].append(time.time())\n",
    "            output_first_channel = int(isolate_background) # if isolate_background, bg's probs (output's channel 0) are not considered\n",
    "            output = torch.argmax(test_outputs[:, output_first_channel:], dim=1)[0].float()\n",
    "            confidence_map = torch.max(test_outputs[:, output_first_channel:], dim=1)[0].float()\n",
    "            time_secs['Output and conf. map'][-1] = time.time() - time_secs['Output and conf. map'][-1]\n",
    "            \n",
    "            if is_step_last_or_multiple_of(step, total_steps = len(test_loader), multiple_of = 50):\n",
    "                time_secs['RGB mapping'].append(time.time())\n",
    "                if binary:\n",
    "                    test_labels_img = test_labels[0]/(n_classes - 1)\n",
    "                    output_img = output/(n_classes - 1 - output_first_channel)\n",
    "                else:\n",
    "                    test_labels_img = generate_rgb_map(test_labels[0].cpu(), is_channel_first = True, as_tensor = True)\n",
    "                    output += output_first_channel\n",
    "                    #output = torch.where(\n",
    "                    #    test_labels[0] == 0,\n",
    "                    #    torch.zeros(output.shape, dtype = output.dtype).to(output.device),\n",
    "                    #    output)\n",
    "                    output_img = generate_rgb_map(output.cpu(), is_channel_first = output.ndim == 3, as_tensor = True)\n",
    "                time_secs['RGB mapping'][-1] = time.time() - time_secs['RGB mapping'][-1]\n",
    "                \n",
    "                if not show_output_comparison:\n",
    "                    time_secs['Aim Image Logging'].append(time.time())\n",
    "                    aim_run.track(aim.Image(test_labels_img, \\\n",
    "                                            caption=f'Label Image: {index}'), \\\n",
    "                                   name='validation', context={'type':'label'})\n",
    "                    aim_run.track(aim.Image(output_img, caption=f'Predicted Label: {index}'), \\\n",
    "                                   name = 'validation', context={'type':'prediction'})\n",
    "                    aim_run.track(aim.Image(test_inputs[0, :3], \\\n",
    "                                            caption=f'Input Image: {index}'), \\\n",
    "                                   name='validation', context={'type':'input'})\n",
    "                    if UNet_metadata['in_channels'] == 6:\n",
    "                        aim_run.track(aim.Image(test_inputs[0, 3:], \\\n",
    "                                            caption=f'Input Image PX: {index}'), \\\n",
    "                                   name='validation', context={'type':'input_PX'})\n",
    "                    aim_run.track(aim.Image(confidence_map/confidence_map.max(), \\\n",
    "                                            caption=f'Input Image: {index}'), \\\n",
    "                                   name='validation', context={'type':'confidence'})\n",
    "\n",
    "                    if binary:\n",
    "                        #diff_output_labels = output.to(torch.int8) - test_labels[0].to(torch.int8)\n",
    "                        input_high_red      = test_inputs[0, :3].clone()\n",
    "                        input_high_red[0]   = (input_high_red[0] + 1)/2\n",
    "                        input_high_green    = test_inputs[0, :3].clone()\n",
    "                        input_high_green[1] = (input_high_green[1] + 1)/2\n",
    "                        input_high_blue     = test_inputs[0, :3].clone()\n",
    "                        input_high_blue[2]  = (input_high_blue[2] + 1)/2\n",
    "\n",
    "                        FN_highlighted       = torch.where(\n",
    "                            (output ==  0).to(torch.bool) & (test_labels[0] == new_pore_label).to(torch.bool),\n",
    "                            input_high_blue,  test_inputs[0, :3])\n",
    "                        FN_TP_highlighted    = torch.where(\n",
    "                            (output ==  1).to(torch.bool) & (test_labels[0] == new_pore_label).to(torch.bool),\n",
    "                            input_high_green, FN_highlighted)\n",
    "                        FN_TP_FP_highlighted = torch.where(\n",
    "                            (output ==  1).to(torch.bool) & (test_labels[0] == new_non_pore_label).to(torch.bool),\n",
    "                            input_high_red,   FN_TP_highlighted)\n",
    "\n",
    "                        aim_run.track(aim.Image(FN_TP_FP_highlighted, \\\n",
    "                                                caption=f'FN (blue); TP (green); FP (red): {index}'), \\\n",
    "                                       name='validation', context={'type':'fn+tp+fp'})\n",
    "\n",
    "                    time_secs['Aim Image Logging'][-1] = time.time() - time_secs['Aim Image Logging'][-1]\n",
    "                else:\n",
    "                    get_comparison(test_inputs[0, :3].cpu(), test_labels_img, output_img, confidence_map, step, show = True)\n",
    "                    print(step, '===', image_path, 10 * '=')\n",
    "\n",
    "            # validation loss\n",
    "            time_secs['Loss calculation'].append(time.time())\n",
    "            loss = loss_function(test_outputs, test_labels)\n",
    "            test_loss += loss.item()\n",
    "            time_secs['Loss calculation'][-1] = time.time() - time_secs['Loss calculation'][-1]\n",
    "\n",
    "            test_outputs = [post_pred(i) for i in decollate_batch(test_outputs)]\n",
    "            test_labels = [post_label(i) for i in decollate_batch(test_labels)]\n",
    "            \n",
    "            # compute metric for current iteration\n",
    "            time_secs['Dice metric'].append(time.time())\n",
    "            dice_metric(y_pred=test_outputs, y=test_labels)\n",
    "            time_secs['Dice metric'][-1] = time.time() - time_secs['Dice metric'][-1]\n",
    "            time_secs['Proportion metric'].append(time.time())\n",
    "            proportion_metric(y_pred=test_outputs, y=test_labels, image_path=image_path)\n",
    "            time_secs['Proportion metric'][-1] = time.time() - time_secs['Proportion metric'][-1]\n",
    "            time_secs['Confusion metric'].append(time.time())\n",
    "            confusion_metric(y_pred=test_outputs, y=test_labels, image_path=image_path)\n",
    "            time_secs['Confusion metric'][-1] = time.time() - time_secs['Confusion metric'][-1]\n",
    "\n",
    "            time_secs['Loading'].append(time.time())\n",
    "        time_secs['Loading'] = time_secs['Loading'][:-1]\n",
    "        \n",
    "        # validation loss\n",
    "        test_loss /= step\n",
    "        \n",
    "        #-## sparse: média dos erros calculados por imagem;\n",
    "        #-## dense:  erro calculado para o conjunto de validação completo, como uma única grande imagem.\n",
    "        #-##         Baseado no fato de que as proporções de cada elemento em uma imagem completa é a média das proporções\n",
    "        #-##         em cada subimagem (quando todas as subimagens têm tamanhos iguais).\n",
    "        #-#dense_proportions = {\n",
    "        #-#    'val': test_proportions.mean(dim = 0),\n",
    "        #-#    'out': output_proportions.mean(dim = 0)\n",
    "        #-#}\n",
    "        \n",
    "        # aggregate the final mean dice result\n",
    "        time_secs['Dice aggregation'] = time.time()\n",
    "        metric = dice_metric.aggregate().item()\n",
    "        time_secs['Dice aggregation'] = time.time() - time_secs['Dice aggregation']\n",
    "        time_secs['Proportion aggregation'] = time.time()\n",
    "        proportion = proportion_metric.aggregate()\n",
    "        time_secs['Proportion aggregation'] = time.time() - time_secs['Proportion aggregation']\n",
    "        time_secs['Confusion aggregation'] = time.time()\n",
    "        confusion = confusion_metric.aggregate()\n",
    "        time_secs['Confusion aggregation'] = time.time() - time_secs['Confusion aggregation']\n",
    "        \n",
    "        if not show_output_comparison:\n",
    "            # track val\n",
    "            aim_run.track(test_loss,  name=(prefix + \"loss\"),   context={'type':loss_type})\n",
    "            aim_run.track(metric,     name=(prefix + \"metric\"), context={'type':loss_type})\n",
    "            aim_run.track(proportion['total']['R2'], name=\"proportion_R2\", context={'type':'Proportion'})\n",
    "            aim_run.track(proportion['total']['RMSE'],  name=\"proportion_linear_RMSE\",  context={'type':'Proportion'})\n",
    "\n",
    "        # reset the status for next validation round\n",
    "        dice_metric.reset()\n",
    "        proportion_metric.reset()\n",
    "        confusion_metric.reset()\n",
    "\n",
    "        track_custom_metrics = False\n",
    "        if validation:\n",
    "            if metric > best_metric:\n",
    "                time_secs['Model saving'] = time.time()\n",
    "                track_custom_metrics = True\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_proportion = proportion\n",
    "                best_confusion = confusion\n",
    "                os.makedirs(os.path.join(project, 'models'), exist_ok = True)\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    project, 'models', aim_run.name.split()[-1] + '.pth'))\n",
    "                time_secs['Model saving'] = time.time() - time_secs['Model saving']\n",
    "                \n",
    "                best_model_log_message = f\"saved new best metric model at the {epoch+1}th epoch\"\n",
    "                aim_run.track(aim.Text(best_model_log_message), name='best_model_log_message', epoch=epoch+1)\n",
    "                print(best_model_log_message)\n",
    "\n",
    "            message1 = f\"current epoch: {epoch + 1} | \" + prefix + f\" loss: {test_loss:.4f} | current mean dice: {metric:.4f}\"\n",
    "            message2 = f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "            message3 = f\"at epoch: {best_metric_epoch}\"\n",
    "\n",
    "            aim_run.track(aim.Text(message1 +\"\\n\" + message2 + message3), name='epoch_summary', epoch=epoch+1)\n",
    "        else:\n",
    "            best_proportion = proportion\n",
    "            best_confusion = confusion\n",
    "            message1 = prefix + f\" loss: {test_loss:.4f}\"\n",
    "            message2 = f\"\\nmean dice: {metric:.4f} \"\n",
    "            message3 = ''\n",
    "        \n",
    "        time_secs['Metrics logging'] = time.time()\n",
    "        for section_id in proportion.keys():\n",
    "            metrics_log = {\n",
    "                'elements': elements,\n",
    "                prefix + 'proportions': proportion[section_id]['true'].cpu().tolist(),\n",
    "                'out_proportions': proportion[section_id]['pred'].cpu().tolist(),\n",
    "                'color_hex': (['#000000'] if 'Desconhecido' in elements else []) + element_data['color_hex'].tolist()\n",
    "            }\n",
    "            \n",
    "            for i, element in enumerate(elements):\n",
    "                metrics_log['Pred. ' + element] = confusion[section_id][:, i].cpu().tolist()\n",
    "            model_name = (aim_run.name if (model_to_load is None) else model_to_load).replace('.pth', '')\n",
    "            if model_name.startswith('Run:'):\n",
    "                model_name = model_name[5:]\n",
    "            log_path = os.path.join(project, 'models_log', model_name, section_id)\n",
    "            os.makedirs(log_path, exist_ok = True)\n",
    "            pd.DataFrame(metrics_log).to_csv(os.path.join(log_path, 'log.csv'), index = False)\n",
    "        time_secs['Metrics logging'] = time.time() - time_secs['Metrics logging']\n",
    "        \n",
    "        if aim_run is not None:\n",
    "            aim_run.track(aim.Distribution(proportion['total']['true']), name=(prefix + 'proportion'), context={'type':'Proportion'})\n",
    "            aim_run.track(aim.Distribution(proportion['total']['pred']), name='out_proportion', context={'type':'Proportion'})\n",
    "            aim_run.track(aim.Image(confusion['total'], caption='Confusion Matrix'), name='confusion_matrix', context={'type':'ConfusionMatrix'})\n",
    "        \n",
    "        print(message1, message2, message3)\n",
    "        time_secs['Plot'] = time.time()\n",
    "        fig = plot_dense_proportions(elements, proportion['total'])\n",
    "        #aim_run.track(aim.Figure(fig), name='proportions', context={'type':'Proportion'})\n",
    "        time_secs['Plot'] = time.time() - time_secs['Plot']\n",
    "        time_secs['Total'] = time.time() - time_secs['Total']\n",
    "        \n",
    "        print('Time:')\n",
    "        for key in time_secs:\n",
    "            if type(time_secs[key]) == list:\n",
    "                time_to_show = np.sum(time_secs[key]).astype(int)\n",
    "            else:\n",
    "                time_to_show = time_secs[key]\n",
    "            print('===', key + ':', time.strftime('%Hh%Mm%Ss', time.gmtime(time_to_show)), end = ' ')\n",
    "        print()\n",
    "        \n",
    "        return best_metric, best_metric_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "test_images = ['5716.70', '5607.35', '5639.30']\n",
    "test_files = []\n",
    "for vf in val_files:\n",
    "    if any(ti in vf['image'] for ti in test_images):\n",
    "        test_files.append(vf)\n",
    "        \n",
    "test_loader = DataLoader(\n",
    "    CacheDataset(data = test_files, transform = val_transforms, cache_rate = 1.0),\n",
    "    batch_size = 1\n",
    ")\n",
    "test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KayxFseYwCMR",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aim_run = None\n",
    "if model_to_load is None or not show_output_comparison:\n",
    "    # initialize a new Aim Run\n",
    "    #experiment = 'sections={train=' + train_sections + ',val=' + val_sections + '}' \\\n",
    "    #                  + '_usePolarizedLight=' + str(not pp_only) \\\n",
    "    #                  + '_PoreVsNonPore=' + str(binary)\n",
    "    aim_run = aim.Run(experiment = experiment, repo = os.path.join(project, 'logging'))\n",
    "\n",
    "train_time_secs = 0\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "if model_to_load is None:\n",
    "    epoch_loss_values = []\n",
    "    \n",
    "    # log model metadata\n",
    "    aim_run['UNet_metadata'] = UNet_metadata\n",
    "    # log optimizer metadata\n",
    "    aim_run['Optimizer_metadata'] = Optimizer_metadata\n",
    "\n",
    "    aim_run['dataset'] = dict(dataset_log)\n",
    "    \n",
    "    aim_run['n_epochs'] = max_epochs\n",
    "    aim_run['img_size'] = img_size\n",
    "    aim_run['crop_spatial_size'] = crop_spatial_size\n",
    "    aim_run['num_crops_per_img'] = num_crops_per_img\n",
    "    aim_run['binary'] = binary\n",
    "    aim_run['use_polarized_light'] = not pp_only\n",
    "    aim_run['shrank'] = dict(shrank)\n",
    "    aim_run['isolate_background'] = isolate_background\n",
    "    aim_run['include_background'] = dict(include_background)\n",
    "    aim_run['weight_samples'] = dict(weight_samples)\n",
    "    aim_run['balance_by_oversampling'] = balance_by_oversampling\n",
    "\n",
    "    log_train_transforms = log_transform(train_transforms)\n",
    "    log_val_transforms   = log_transform(val_transforms)\n",
    "    aim_run['train_transforms'] = dict(functions = str(list(log_train_transforms.keys())), params = str(log_train_transforms))\n",
    "    aim_run['val_transforms']   = dict(functions = str(list(log_val_transforms.keys())),   params = str(log_val_transforms))\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        start_time_secs = time.time()\n",
    "        #if epoch == 5:\n",
    "        #    val_interval = 100\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        n_batches = len(train_ds) // train_loader.batch_size\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = (\n",
    "                batch_data[\"image\"],#.to(device),\n",
    "                batch_data[\"label\"],#.to(device),\n",
    "            )\n",
    "            \n",
    "            #plt.imshow(generate_rgb_map(labels[0], is_channel_first = True))\n",
    "            #plt.show()\n",
    "            \n",
    "            if not multigpu:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels.to(outputs.device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            if step % (n_batches//10) == 0:\n",
    "                print(f\"{step}/{n_batches}, \"\n",
    "                    f\"train_loss: {loss.item():.4f}\")\n",
    "            # track batch loss metric\n",
    "            aim_run.track(loss.item(), name=\"batch_loss\", context={'type':loss_type})\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "        # track epoch loss metric\n",
    "        aim_run.track(epoch_loss, name=\"epoch_loss\", context={'type':loss_type})\n",
    "\n",
    "        epoch_time_secs  = time.time() - start_time_secs\n",
    "        train_time_secs += epoch_time_secs\n",
    "        \n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        print('=== Epoch time:', time.strftime('%Hh%Mm%Ss', time.gmtime(epoch_time_secs)), end = ' ')\n",
    "        print('=== Total time:', time.strftime('%Hh%Mm%Ss', time.gmtime(train_time_secs)), end = ' ')\n",
    "        print('=== ETA:', time.strftime('%Hh%Mm%Ss', time.gmtime((max_epochs - (epoch + 1)) * epoch_time_secs)))\n",
    "\n",
    "        if val_interval is not None:\n",
    "            if (epoch + 1) % val_interval == 0:\n",
    "                if (epoch + 1) % (val_interval * 2) == 0:\n",
    "                    print('Tracking parameters and gradients...')\n",
    "                    # track model params and gradients\n",
    "                    track_params_dists(model,aim_run)\n",
    "                    # THIS SEGMENT TAKES RELATIVELY LONG (Advise Against it)\n",
    "                    track_gradients_dists(model, aim_run)\n",
    "\n",
    "                best_metric, best_metric_epoch = test(\n",
    "                    test_loader = val_loader, best_metric = best_metric, best_metric_epoch = best_metric_epoch,\n",
    "                    validation = True\n",
    "                )\n",
    "                \n",
    "    if val_interval is None:\n",
    "        if aim_run is not None:\n",
    "            model_name = aim_run.name.split()[-1]\n",
    "        else:\n",
    "            from datetime import datetime\n",
    "            model_name = experiment + '_' + str(datetime.now())\n",
    "            \n",
    "        torch.save(model.state_dict(), os.path.join(project, 'models', model_name + '.pth'))\n",
    "\n",
    "else:\n",
    "    model.load_state_dict(\n",
    "        torch.load(os.path.join(project, 'models', model_to_load), map_location = device)\n",
    "    )\n",
    "    if not show_output_comparison:\n",
    "        aim_run['model'] = model_to_load\n",
    "    test(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1WMn7DFKkbV"
   },
   "outputs": [],
   "source": [
    "if aim_run is not None:\n",
    "    # finalize Aim Run\n",
    "    aim_run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90la_Hq8wCMR"
   },
   "source": [
    "## Aim UI diretamente no notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGhCvBg-wCMS",
    "scrolled": false
   },
   "source": [
    "%load_ext aim\n",
    "%aim up"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "spleen_segmentation_3d_visualization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
